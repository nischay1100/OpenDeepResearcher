{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO+gBHdCOEPmWE4QAiiSNWe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nischay1100/OpenDeepResearcher/blob/main/Milestone1_ScopePhase.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cell 1 — Install required libraries"
      ],
      "metadata": {
        "id": "eS6VmwpQxjTD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xSJfG3aTxWoa"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade langgraph google-generativeai tavily-python --quiet"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Imports and API key configuration"
      ],
      "metadata": {
        "id": "K8_E7nvvx41H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from getpass import getpass\n",
        "import json, re, traceback, time\n",
        "from typing import TypedDict\n",
        "\n",
        "# LangGraph StateGraph\n",
        "from langgraph.graph import StateGraph\n",
        "\n",
        "# Google Gemini client\n",
        "import google.generativeai as genai\n",
        "\n",
        "# Tavily client\n",
        "from tavily import TavilyClient\n",
        "\n",
        "# Enter API keys\n",
        "GENAI_API_KEY = getpass(\"Enter Google Gemini API Key: \")\n",
        "TAVILY_API_KEY = getpass(\"Enter Tavily API Key: \")\n",
        "\n",
        "# Configure clients\n",
        "genai.configure(api_key=GENAI_API_KEY)\n",
        "tavily = TavilyClient(api_key=TAVILY_API_KEY)\n"
      ],
      "metadata": {
        "id": "eshOywZzx6Hf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cell 3 — State type definition"
      ],
      "metadata": {
        "id": "d21LxyXwyHAS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the state structure for LangGraph nodes\n",
        "class ResearchState(TypedDict, total=False):\n",
        "    user_input: str\n",
        "    clarification: str\n",
        "    query: str\n",
        "    summary: str\n",
        "    pipeline: str\n"
      ],
      "metadata": {
        "id": "mJEsNQXlyLpL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cell 4 — Utility: safe response extractor"
      ],
      "metadata": {
        "id": "jcCWDDS7yqrj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Utility to safely extract text from Google generative responses\n",
        "def safe_extract_genai_text(response):\n",
        "    \"\"\"\n",
        "    Given a genai response object, try several known access patterns and return plain text.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # preferred: genai response may have .text\n",
        "        if hasattr(response, \"text\") and response.text:\n",
        "            return response.text.strip()\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    try:\n",
        "        # older interface: candidates --> content --> parts\n",
        "        cand = response.candidates[0]\n",
        "        # try direct text\n",
        "        if hasattr(cand, \"content\"):\n",
        "            # some libs put text in content.parts[0].text\n",
        "            content = getattr(cand, \"content\", None)\n",
        "            if content and hasattr(content, \"parts\"):\n",
        "                parts = content.parts\n",
        "                if parts and len(parts) > 0 and getattr(parts[0], \"text\", None):\n",
        "                    return parts[0].text.strip()\n",
        "        # fallback to candidate.text\n",
        "        if hasattr(cand, \"text\") and cand.text:\n",
        "            return cand.text.strip()\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    # Last resort: stringify\n",
        "    try:\n",
        "        return str(response)\n",
        "    except:\n",
        "        return \"\"\n"
      ],
      "metadata": {
        "id": "u_CoTHT9yr8p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cell 5 — Clarification agent"
      ],
      "metadata": {
        "id": "b40a5Jw3y9HQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Clarification agent\n",
        "def clarification_agent(state: ResearchState) -> ResearchState:\n",
        "    user_input = state.get(\"user_input\", \"\").strip()\n",
        "    if not user_input:\n",
        "        state[\"clarification\"] = \"Could you type your question?\"\n",
        "        return state\n",
        "\n",
        "    # Fast rule-based checks\n",
        "    if re.search(r\"\\bmy name is\\b\", user_input.lower()):\n",
        "        state[\"clarification\"] = \"This request is clear\"\n",
        "        return state\n",
        "\n",
        "    if re.search(r\"\\b(previous|last)\\s*(que|question|query|sawal)\\b\", user_input.lower()):\n",
        "        state[\"clarification\"] = \"This request is clear\"\n",
        "        return state\n",
        "\n",
        "    # Ask Gemini to categorize the clarity\n",
        "    prompt = f\"\"\"\n",
        "You are a system that assesses clarity of user research questions.\n",
        "\n",
        "Question: \"{user_input}\"\n",
        "\n",
        "Classify the question into one of:\n",
        "- clear\n",
        "- vague_guessable\n",
        "- too_vague\n",
        "\n",
        "If you return \"vague_guessable\", provide a short \"refined_question\" that is a reasonable interpretation.\n",
        "Respond ONLY in JSON with keys: \"status\" and \"refined_question\" (string or empty).\n",
        "Example:\n",
        "{{\"status\":\"vague_guessable\", \"refined_question\":\"...\"}}\n",
        "\"\"\"\n",
        "\n",
        "    try:\n",
        "        model = genai.GenerativeModel(\"gemini-1.5-flash\")\n",
        "        response = model.generate_content(prompt)\n",
        "        text_out = safe_extract_genai_text(response)\n",
        "        # attempt to parse JSON from the response\n",
        "        parsed = {}\n",
        "        try:\n",
        "            parsed = json.loads(text_out)\n",
        "        except Exception:\n",
        "            # try to find a JSON substring\n",
        "            m = re.search(r\"\\{.*\\}\", text_out, flags=re.DOTALL)\n",
        "            if m:\n",
        "                try:\n",
        "                    parsed = json.loads(m.group(0))\n",
        "                except:\n",
        "                    parsed = {}\n",
        "        status = parsed.get(\"status\", \"\").lower()\n",
        "        refined = parsed.get(\"refined_question\", \"\").strip()\n",
        "    except Exception:\n",
        "        traceback.print_exc()\n",
        "        status, refined = \"clear\", \"\"\n",
        "\n",
        "    if status == \"clear\":\n",
        "        state[\"clarification\"] = \"This request is clear\"\n",
        "    elif status == \"vague_guessable\" and refined:\n",
        "        state[\"clarification\"] = refined\n",
        "    else:\n",
        "        state[\"clarification\"] = \"Could you provide more details about your question?\"\n",
        "\n",
        "    return state\n"
      ],
      "metadata": {
        "id": "AyhznmOiy7Qh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cell 6 — Query generator node"
      ],
      "metadata": {
        "id": "j05Ce4PpzLez"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Query generator\n",
        "def query_generator(state: ResearchState) -> ResearchState:\n",
        "    clarification = state.get(\"clarification\", \"\")\n",
        "    user_input = state.get(\"user_input\", \"\")\n",
        "\n",
        "    if clarification == \"This request is clear\":\n",
        "        state[\"query\"] = user_input\n",
        "    elif clarification.startswith(\"Could you provide\"):\n",
        "        state[\"query\"] = f\"{user_input} (needs clarification: {clarification})\"\n",
        "    else:\n",
        "        # If we have a refined phrasing, use it\n",
        "        state[\"query\"] = clarification or user_input\n",
        "\n",
        "    return state\n"
      ],
      "metadata": {
        "id": "sTFV0crgzJVp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cell 7 — Search decision, Tavily wrapper, and research pipeline"
      ],
      "metadata": {
        "id": "70S5kithze4P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# decide_search + tavily wrapper + research pipeline\n",
        "def decide_search(query: str) -> bool:\n",
        "    \"\"\"Ask Gemini whether a web search is required. Returns True if search is needed.\"\"\"\n",
        "    try:\n",
        "        prompt = f\"\"\"\n",
        "You are a decision module. Given a research question, answer whether it requires real-time web search\n",
        "or can be answered from general knowledge (no web search). Return JSON: {{\"need_search\": true/false}}.\n",
        "\n",
        "Question: \"{query}\"\n",
        "\"\"\"\n",
        "        model = genai.GenerativeModel(\"gemini-1.5-flash\")\n",
        "        response = model.generate_content(prompt)\n",
        "        text_out = safe_extract_genai_text(response)\n",
        "        # parse JSON\n",
        "        parsed = {}\n",
        "        try:\n",
        "            parsed = json.loads(text_out)\n",
        "        except:\n",
        "            m = re.search(r\"\\{.*\\}\", text_out, flags=re.DOTALL)\n",
        "            if m:\n",
        "                try:\n",
        "                    parsed = json.loads(m.group(0))\n",
        "                except:\n",
        "                    parsed = {}\n",
        "        return bool(parsed.get(\"need_search\", True))\n",
        "    except Exception:\n",
        "        return True  # conservative default\n",
        "\n",
        "def tavily_search(query: str, max_results: int = 5):\n",
        "    \"\"\"\n",
        "    Wrapper for TavilyClient search.\n",
        "    Returns a list of result dicts or a string description if the client isn't available.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # The tavily client API may differ between versions; try a few common names.\n",
        "        if hasattr(tavily, \"search\"):\n",
        "            results = tavily.search(query, max_results=max_results)\n",
        "            return results\n",
        "        elif hasattr(tavily, \"query\"):\n",
        "            results = tavily.query(query, max_results=max_results)\n",
        "            return results\n",
        "        else:\n",
        "            # fallback: call a generic 'run' if present\n",
        "            if hasattr(tavily, \"run\"):\n",
        "                return tavily.run(query)\n",
        "            # if none of the above, return a placeholder\n",
        "            return f\"[Tavily client available but no known search method; query='{query}']\"\n",
        "    except Exception as e:\n",
        "        return f\"[Tavily search error: {str(e)}]\"\n",
        "\n",
        "def research_pipeline(state: ResearchState) -> ResearchState:\n",
        "    \"\"\"\n",
        "    Main research node - decides memory shortcuts, whether to use web search,\n",
        "    and produces a short summary (or placeholder).\n",
        "    \"\"\"\n",
        "    global memory\n",
        "    query = (state.get(\"query\") or \"\").strip()\n",
        "    if not query:\n",
        "        state[\"pipeline\"] = \"No query provided.\"\n",
        "        state[\"summary\"] = \"No summary available.\"\n",
        "        return state\n",
        "\n",
        "    # Memory shortcuts\n",
        "    if re.search(r\"\\b(previous|last)\\s*(que|question|query|sawal)\\b\", query.lower()):\n",
        "        if memory[\"history\"]:\n",
        "            last_q = memory[\"history\"][-1][\"Q\"]\n",
        "            state[\"pipeline\"] = \"Retrieved from memory (history).\"\n",
        "            state[\"summary\"] = f\"Your previous question was: '{last_q}'\"\n",
        "        else:\n",
        "            state[\"pipeline\"] = \"Memory empty.\"\n",
        "            state[\"summary\"] = \"There is no previous question in memory.\"\n",
        "        return state\n",
        "\n",
        "    if \"my name\" in query.lower():\n",
        "        name = memory[\"facts\"].get(\"name\", \"I don’t know yet.\")\n",
        "        state[\"pipeline\"] = \"Retrieved from memory (facts).\"\n",
        "        state[\"summary\"] = f\"Your name is {name}.\"\n",
        "        return state\n",
        "\n",
        "    # Direct fact lookup\n",
        "    facts = memory.get(\"facts\", {})\n",
        "    if query.lower() in (k.lower() for k in facts.keys()):\n",
        "        matched = next((v for k, v in facts.items() if k.lower() == query.lower()), None)\n",
        "        state[\"pipeline\"] = f\"Retrieved from memory: {matched}\"\n",
        "        state[\"summary\"] = matched\n",
        "        return state\n",
        "\n",
        "    # Decide whether to use web search\n",
        "    need_search = decide_search(query)\n",
        "\n",
        "    model = genai.GenerativeModel(\"gemini-1.5-flash\")\n",
        "    try:\n",
        "        if need_search:\n",
        "            # Get web data\n",
        "            tavily_results = tavily_search(query, max_results=5)\n",
        "            # Combine into a short context for Gemini to summarize\n",
        "            combined_info = f\"Query: {query}\\n\\nWeb results (short): {json.dumps(tavily_results, default=str)[:4000]}\\n\\nSummarize the key findings in 3-5 bullet points.\"\n",
        "            response = model.generate_content(combined_info)\n",
        "            text_out = safe_extract_genai_text(response)\n",
        "            state[\"pipeline\"] = \"Tavily + Gemini\"\n",
        "            state[\"summary\"] = text_out\n",
        "        else:\n",
        "            # Use Gemini directly\n",
        "            prompt = f\"Query: {query}\\nProvide a concise answer or short summary (3-5 lines).\"\n",
        "            response = model.generate_content(prompt)\n",
        "            text_out = safe_extract_genai_text(response)\n",
        "            state[\"pipeline\"] = \"Gemini Only\"\n",
        "            state[\"summary\"] = text_out\n",
        "    except Exception as e:\n",
        "        traceback.print_exc()\n",
        "        state[\"pipeline\"] = f\"Error during research: {str(e)}\"\n",
        "        state[\"summary\"] = \"An error occurred while fetching results.\"\n",
        "\n",
        "    return state\n"
      ],
      "metadata": {
        "id": "T3pedJ6_zrUv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cell 8 — Build LangGraph StateGraph and compile"
      ],
      "metadata": {
        "id": "p--gWOi-0J5L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the StateGraph\n",
        "graph = StateGraph(ResearchState)\n",
        "\n",
        "# Add nodes\n",
        "graph.add_node(\"ClarificationAgent\", clarification_agent)\n",
        "graph.add_node(\"QueryGenerator\", query_generator)\n",
        "graph.add_node(\"ResearchPipeline\", research_pipeline)\n",
        "\n",
        "# Define flow\n",
        "graph.set_entry_point(\"ClarificationAgent\")\n",
        "graph.add_edge(\"ClarificationAgent\", \"QueryGenerator\")\n",
        "graph.add_edge(\"QueryGenerator\", \"ResearchPipeline\")\n",
        "\n",
        "# Compile the graph into an app object\n",
        "app = graph.compile()\n",
        "print(\"✅ StateGraph compiled successfully.\")\n"
      ],
      "metadata": {
        "id": "Jf_eFcEq0L3X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cell 9 — Memory initialization and chat() function (single-turn)"
      ],
      "metadata": {
        "id": "cRB3NmoV0ZWh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize memory and provide a chat() function to process one input at a time\n",
        "memory = {\n",
        "    \"facts\": {},    # Persistent knowledge (like user name, facts)\n",
        "    \"history\": []   # Conversation log\n",
        "}\n",
        "\n",
        "def extract_facts_with_gemini(text: str):\n",
        "    \"\"\"\n",
        "    Use Gemini to extract personal facts in JSON-list format: [{\"key\":\"...\", \"value\":\"...\"}]\n",
        "    Fallbacks are safe and non-fatal.\n",
        "    \"\"\"\n",
        "    model = genai.GenerativeModel(\"gemini-1.5-flash\")\n",
        "    prompt = f\"\"\"\n",
        "Extract any personal facts (name, age, location, role, company) from the following user sentence.\n",
        "Return a JSON list of objects with \"key\" and \"value\". If none, return [].\n",
        "\n",
        "Sentence: \"{text}\"\n",
        "\"\"\"\n",
        "    try:\n",
        "        response = model.generate_content(prompt)\n",
        "        text_out = safe_extract_genai_text(response)\n",
        "        # try to parse\n",
        "        facts = []\n",
        "        try:\n",
        "            facts = json.loads(text_out)\n",
        "        except:\n",
        "            # try to find JSON array substring\n",
        "            m = re.search(r\"\\[.*\\]\", text_out, flags=re.DOTALL)\n",
        "            if m:\n",
        "                try:\n",
        "                    facts = json.loads(m.group(0))\n",
        "                except:\n",
        "                    facts = []\n",
        "        if not isinstance(facts, list):\n",
        "            facts = []\n",
        "        return facts\n",
        "    except Exception:\n",
        "        return []\n",
        "\n",
        "def chat(user_input: str, remember_name_rule: bool = True):\n",
        "    \"\"\"\n",
        "    Process a single user_input through the scope pipeline.\n",
        "    Returns the final state dict.\n",
        "    \"\"\"\n",
        "    global memory\n",
        "\n",
        "    # 1) Quick custom name rule\n",
        "    if remember_name_rule and re.search(r\"\\bmy name is\\b\", user_input.lower()):\n",
        "        # extract the phrase after \"my name is\"\n",
        "        try:\n",
        "            name = user_input.lower().split(\"my name is\", 1)[1].strip().split()[0]\n",
        "            memory[\"facts\"][\"name\"] = name.capitalize()\n",
        "            print(f\"✅ Stored name='{memory['facts']['name']}' in memory.\")\n",
        "        except Exception:\n",
        "            pass\n",
        "    else:\n",
        "        # 1b) Use Gemini to detect facts\n",
        "        try:\n",
        "            facts_list = extract_facts_with_gemini(user_input)\n",
        "            for f in facts_list:\n",
        "                key = f.get(\"key\", \"\").lower().strip()\n",
        "                value = f.get(\"value\", \"\").strip()\n",
        "                if key and value:\n",
        "                    memory[\"facts\"][key] = value\n",
        "                    print(f\"✅ I'll remember your {key} = {value}\")\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "    # 2) Handle simple recall commands locally\n",
        "    if user_input.lower().startswith(\"what is my\"):\n",
        "        key = user_input.lower().replace(\"what is my\", \"\").strip()\n",
        "        val = memory[\"facts\"].get(key, \"I don’t know yet.\")\n",
        "        print(f\"Memory: {val}\")\n",
        "        return {\"user_input\": user_input, \"clarification\": \"\", \"query\": \"\", \"pipeline\": \"recall\", \"summary\": val}\n",
        "\n",
        "    # 3) Create state and invoke the pipeline (LangGraph app)\n",
        "    state: ResearchState = {\n",
        "        \"user_input\": user_input,\n",
        "        \"clarification\": \"\",\n",
        "        \"query\": \"\",\n",
        "        \"summary\": \"\",\n",
        "        \"pipeline\": \"\"\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        state = app.invoke(state)\n",
        "    except Exception as e:\n",
        "        traceback.print_exc()\n",
        "        state[\"pipeline\"] = f\"Graph invocation error: {str(e)}\"\n",
        "        state[\"summary\"] = \"\"\n",
        "\n",
        "    # 4) Show outputs\n",
        "    print(\"\\n### 🟢 Clarification Agent\")\n",
        "    print(state.get(\"clarification\", \"\"))\n",
        "    print(\"\\n### 📌 Final Research Query\")\n",
        "    print(state.get(\"query\", \"\"))\n",
        "    print(\"\\n### 🔎 Research Pipeline\")\n",
        "    print(state.get(\"pipeline\", \"\"))\n",
        "    print(\"\\n### ✅ Final Summary\")\n",
        "    print(state.get(\"summary\", \"\"))\n",
        "\n",
        "    # 5) Save to memory/history\n",
        "    memory[\"history\"].append({\n",
        "        \"timestamp\": time.time(),\n",
        "        \"Q\": user_input,\n",
        "        \"clarification\": state.get(\"clarification\", \"\"),\n",
        "        \"query\": state.get(\"query\", \"\"),\n",
        "        \"pipeline\": state.get(\"pipeline\", \"\"),\n",
        "        \"A\": state.get(\"summary\", \"\")\n",
        "    })\n",
        "\n",
        "    return state\n"
      ],
      "metadata": {
        "id": "QRdsfsu90XGu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cell 10 — continuous chatbot loop"
      ],
      "metadata": {
        "id": "vSZKmyI50xl0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Continuous chatbot loop\n",
        "print(\"🟢 OpenDeepResearcher Chatbot (type 'quit' or 'exit' to stop)\\n\")\n",
        "\n",
        "while True:\n",
        "    try:\n",
        "        user_input = input(\"You: \").strip()\n",
        "        if user_input.lower() in [\"quit\", \"exit\", \"bye\"]:\n",
        "            print(\"👋 Goodbye! Session ended.\")\n",
        "            break\n",
        "\n",
        "        # Process the user input through our chat() pipeline\n",
        "        state = chat(user_input)\n",
        "\n",
        "    except KeyboardInterrupt:\n",
        "        print(\"\\n👋 Interrupted. Goodbye!\")\n",
        "        break\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ Error: {e}\")\n"
      ],
      "metadata": {
        "id": "ixntL6zZ01uz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}