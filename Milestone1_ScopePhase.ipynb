{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO+gBHdCOEPmWE4QAiiSNWe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nischay1100/OpenDeepResearcher/blob/main/Milestone1_ScopePhase.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cell 1 — Install required libraries"
      ],
      "metadata": {
        "id": "eS6VmwpQxjTD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xSJfG3aTxWoa",
        "outputId": "4e3b1ffa-c148-47a1-9258-d9db16187468"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m153.3/153.3 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.9/43.9 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.1/56.1 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m216.7/216.7 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade langgraph google-generativeai tavily-python --quiet"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Imports and API key configuration"
      ],
      "metadata": {
        "id": "K8_E7nvvx41H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from getpass import getpass\n",
        "import json, re, traceback, time\n",
        "from typing import TypedDict\n",
        "\n",
        "# LangGraph StateGraph\n",
        "from langgraph.graph import StateGraph\n",
        "\n",
        "# Google Gemini client\n",
        "import google.generativeai as genai\n",
        "\n",
        "# Tavily client\n",
        "from tavily import TavilyClient\n",
        "\n",
        "# Enter API keys\n",
        "GENAI_API_KEY = getpass(\"Enter Google Gemini API Key: \")\n",
        "TAVILY_API_KEY = getpass(\"Enter Tavily API Key: \")\n",
        "\n",
        "# Configure clients\n",
        "genai.configure(api_key=GENAI_API_KEY)\n",
        "tavily = TavilyClient(api_key=TAVILY_API_KEY)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eshOywZzx6Hf",
        "outputId": "1ca6db7d-9239-4b6a-e67d-3ea5d5da3c51"
      },
      "execution_count": 2,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter Google Gemini API Key: ··········\n",
            "Enter Tavily API Key: ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cell 3 — State type definition"
      ],
      "metadata": {
        "id": "d21LxyXwyHAS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the state structure for LangGraph nodes\n",
        "class ResearchState(TypedDict, total=False):\n",
        "    user_input: str\n",
        "    clarification: str\n",
        "    query: str\n",
        "    summary: str\n",
        "    pipeline: str\n"
      ],
      "metadata": {
        "id": "mJEsNQXlyLpL"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cell 4 — Utility: safe response extractor"
      ],
      "metadata": {
        "id": "jcCWDDS7yqrj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Utility to safely extract text from Google generative responses\n",
        "def safe_extract_genai_text(response):\n",
        "    \"\"\"\n",
        "    Given a genai response object, try several known access patterns and return plain text.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # preferred: genai response may have .text\n",
        "        if hasattr(response, \"text\") and response.text:\n",
        "            return response.text.strip()\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    try:\n",
        "        # older interface: candidates --> content --> parts\n",
        "        cand = response.candidates[0]\n",
        "        # try direct text\n",
        "        if hasattr(cand, \"content\"):\n",
        "            # some libs put text in content.parts[0].text\n",
        "            content = getattr(cand, \"content\", None)\n",
        "            if content and hasattr(content, \"parts\"):\n",
        "                parts = content.parts\n",
        "                if parts and len(parts) > 0 and getattr(parts[0], \"text\", None):\n",
        "                    return parts[0].text.strip()\n",
        "        # fallback to candidate.text\n",
        "        if hasattr(cand, \"text\") and cand.text:\n",
        "            return cand.text.strip()\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    # Last resort: stringify\n",
        "    try:\n",
        "        return str(response)\n",
        "    except:\n",
        "        return \"\"\n"
      ],
      "metadata": {
        "id": "u_CoTHT9yr8p"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cell 5 — Clarification agent"
      ],
      "metadata": {
        "id": "b40a5Jw3y9HQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Clarification agent\n",
        "def clarification_agent(state: ResearchState) -> ResearchState:\n",
        "    user_input = state.get(\"user_input\", \"\").strip()\n",
        "    if not user_input:\n",
        "        state[\"clarification\"] = \"Could you type your question?\"\n",
        "        return state\n",
        "\n",
        "    # Fast rule-based checks\n",
        "    if re.search(r\"\\bmy name is\\b\", user_input.lower()):\n",
        "        state[\"clarification\"] = \"This request is clear\"\n",
        "        return state\n",
        "\n",
        "    if re.search(r\"\\b(previous|last)\\s*(que|question|query|sawal)\\b\", user_input.lower()):\n",
        "        state[\"clarification\"] = \"This request is clear\"\n",
        "        return state\n",
        "\n",
        "    # Ask Gemini to categorize the clarity\n",
        "    prompt = f\"\"\"\n",
        "You are a system that assesses clarity of user research questions.\n",
        "\n",
        "Question: \"{user_input}\"\n",
        "\n",
        "Classify the question into one of:\n",
        "- clear\n",
        "- vague_guessable\n",
        "- too_vague\n",
        "\n",
        "If you return \"vague_guessable\", provide a short \"refined_question\" that is a reasonable interpretation.\n",
        "Respond ONLY in JSON with keys: \"status\" and \"refined_question\" (string or empty).\n",
        "Example:\n",
        "{{\"status\":\"vague_guessable\", \"refined_question\":\"...\"}}\n",
        "\"\"\"\n",
        "\n",
        "    try:\n",
        "        model = genai.GenerativeModel(\"gemini-1.5-flash\")\n",
        "        response = model.generate_content(prompt)\n",
        "        text_out = safe_extract_genai_text(response)\n",
        "        # attempt to parse JSON from the response\n",
        "        parsed = {}\n",
        "        try:\n",
        "            parsed = json.loads(text_out)\n",
        "        except Exception:\n",
        "            # try to find a JSON substring\n",
        "            m = re.search(r\"\\{.*\\}\", text_out, flags=re.DOTALL)\n",
        "            if m:\n",
        "                try:\n",
        "                    parsed = json.loads(m.group(0))\n",
        "                except:\n",
        "                    parsed = {}\n",
        "        status = parsed.get(\"status\", \"\").lower()\n",
        "        refined = parsed.get(\"refined_question\", \"\").strip()\n",
        "    except Exception:\n",
        "        traceback.print_exc()\n",
        "        status, refined = \"clear\", \"\"\n",
        "\n",
        "    if status == \"clear\":\n",
        "        state[\"clarification\"] = \"This request is clear\"\n",
        "    elif status == \"vague_guessable\" and refined:\n",
        "        state[\"clarification\"] = refined\n",
        "    else:\n",
        "        state[\"clarification\"] = \"Could you provide more details about your question?\"\n",
        "\n",
        "    return state\n"
      ],
      "metadata": {
        "id": "AyhznmOiy7Qh"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cell 6 — Query generator node"
      ],
      "metadata": {
        "id": "j05Ce4PpzLez"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Query generator\n",
        "def query_generator(state: ResearchState) -> ResearchState:\n",
        "    clarification = state.get(\"clarification\", \"\")\n",
        "    user_input = state.get(\"user_input\", \"\")\n",
        "\n",
        "    if clarification == \"This request is clear\":\n",
        "        state[\"query\"] = user_input\n",
        "    elif clarification.startswith(\"Could you provide\"):\n",
        "        state[\"query\"] = f\"{user_input} (needs clarification: {clarification})\"\n",
        "    else:\n",
        "        # If we have a refined phrasing, use it\n",
        "        state[\"query\"] = clarification or user_input\n",
        "\n",
        "    return state\n"
      ],
      "metadata": {
        "id": "sTFV0crgzJVp"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cell 7 — Search decision, Tavily wrapper, and research pipeline"
      ],
      "metadata": {
        "id": "70S5kithze4P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# decide_search + tavily wrapper + research pipeline\n",
        "def decide_search(query: str) -> bool:\n",
        "    \"\"\"Ask Gemini whether a web search is required. Returns True if search is needed.\"\"\"\n",
        "    try:\n",
        "        prompt = f\"\"\"\n",
        "You are a decision module. Given a research question, answer whether it requires real-time web search\n",
        "or can be answered from general knowledge (no web search). Return JSON: {{\"need_search\": true/false}}.\n",
        "\n",
        "Question: \"{query}\"\n",
        "\"\"\"\n",
        "        model = genai.GenerativeModel(\"gemini-1.5-flash\")\n",
        "        response = model.generate_content(prompt)\n",
        "        text_out = safe_extract_genai_text(response)\n",
        "        # parse JSON\n",
        "        parsed = {}\n",
        "        try:\n",
        "            parsed = json.loads(text_out)\n",
        "        except:\n",
        "            m = re.search(r\"\\{.*\\}\", text_out, flags=re.DOTALL)\n",
        "            if m:\n",
        "                try:\n",
        "                    parsed = json.loads(m.group(0))\n",
        "                except:\n",
        "                    parsed = {}\n",
        "        return bool(parsed.get(\"need_search\", True))\n",
        "    except Exception:\n",
        "        return True  # conservative default\n",
        "\n",
        "def tavily_search(query: str, max_results: int = 5):\n",
        "    \"\"\"\n",
        "    Wrapper for TavilyClient search.\n",
        "    Returns a list of result dicts or a string description if the client isn't available.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # The tavily client API may differ between versions; try a few common names.\n",
        "        if hasattr(tavily, \"search\"):\n",
        "            results = tavily.search(query, max_results=max_results)\n",
        "            return results\n",
        "        elif hasattr(tavily, \"query\"):\n",
        "            results = tavily.query(query, max_results=max_results)\n",
        "            return results\n",
        "        else:\n",
        "            # fallback: call a generic 'run' if present\n",
        "            if hasattr(tavily, \"run\"):\n",
        "                return tavily.run(query)\n",
        "            # if none of the above, return a placeholder\n",
        "            return f\"[Tavily client available but no known search method; query='{query}']\"\n",
        "    except Exception as e:\n",
        "        return f\"[Tavily search error: {str(e)}]\"\n",
        "\n",
        "def research_pipeline(state: ResearchState) -> ResearchState:\n",
        "    \"\"\"\n",
        "    Main research node - decides memory shortcuts, whether to use web search,\n",
        "    and produces a short summary (or placeholder).\n",
        "    \"\"\"\n",
        "    global memory\n",
        "    query = (state.get(\"query\") or \"\").strip()\n",
        "    if not query:\n",
        "        state[\"pipeline\"] = \"No query provided.\"\n",
        "        state[\"summary\"] = \"No summary available.\"\n",
        "        return state\n",
        "\n",
        "    # Memory shortcuts\n",
        "    if re.search(r\"\\b(previous|last)\\s*(que|question|query|sawal)\\b\", query.lower()):\n",
        "        if memory[\"history\"]:\n",
        "            last_q = memory[\"history\"][-1][\"Q\"]\n",
        "            state[\"pipeline\"] = \"Retrieved from memory (history).\"\n",
        "            state[\"summary\"] = f\"Your previous question was: '{last_q}'\"\n",
        "        else:\n",
        "            state[\"pipeline\"] = \"Memory empty.\"\n",
        "            state[\"summary\"] = \"There is no previous question in memory.\"\n",
        "        return state\n",
        "\n",
        "    if \"my name\" in query.lower():\n",
        "        name = memory[\"facts\"].get(\"name\", \"I don’t know yet.\")\n",
        "        state[\"pipeline\"] = \"Retrieved from memory (facts).\"\n",
        "        state[\"summary\"] = f\"Your name is {name}.\"\n",
        "        return state\n",
        "\n",
        "    # Direct fact lookup\n",
        "    facts = memory.get(\"facts\", {})\n",
        "    if query.lower() in (k.lower() for k in facts.keys()):\n",
        "        matched = next((v for k, v in facts.items() if k.lower() == query.lower()), None)\n",
        "        state[\"pipeline\"] = f\"Retrieved from memory: {matched}\"\n",
        "        state[\"summary\"] = matched\n",
        "        return state\n",
        "\n",
        "    # Decide whether to use web search\n",
        "    need_search = decide_search(query)\n",
        "\n",
        "    model = genai.GenerativeModel(\"gemini-1.5-flash\")\n",
        "    try:\n",
        "        if need_search:\n",
        "            # Get web data\n",
        "            tavily_results = tavily_search(query, max_results=5)\n",
        "            # Combine into a short context for Gemini to summarize\n",
        "            combined_info = f\"Query: {query}\\n\\nWeb results (short): {json.dumps(tavily_results, default=str)[:4000]}\\n\\nSummarize the key findings in 3-5 bullet points.\"\n",
        "            response = model.generate_content(combined_info)\n",
        "            text_out = safe_extract_genai_text(response)\n",
        "            state[\"pipeline\"] = \"Tavily + Gemini\"\n",
        "            state[\"summary\"] = text_out\n",
        "        else:\n",
        "            # Use Gemini directly\n",
        "            prompt = f\"Query: {query}\\nProvide a concise answer or short summary (3-5 lines).\"\n",
        "            response = model.generate_content(prompt)\n",
        "            text_out = safe_extract_genai_text(response)\n",
        "            state[\"pipeline\"] = \"Gemini Only\"\n",
        "            state[\"summary\"] = text_out\n",
        "    except Exception as e:\n",
        "        traceback.print_exc()\n",
        "        state[\"pipeline\"] = f\"Error during research: {str(e)}\"\n",
        "        state[\"summary\"] = \"An error occurred while fetching results.\"\n",
        "\n",
        "    return state\n"
      ],
      "metadata": {
        "id": "T3pedJ6_zrUv"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cell 8 — Build LangGraph StateGraph and compile"
      ],
      "metadata": {
        "id": "p--gWOi-0J5L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the StateGraph\n",
        "graph = StateGraph(ResearchState)\n",
        "\n",
        "# Add nodes\n",
        "graph.add_node(\"ClarificationAgent\", clarification_agent)\n",
        "graph.add_node(\"QueryGenerator\", query_generator)\n",
        "graph.add_node(\"ResearchPipeline\", research_pipeline)\n",
        "\n",
        "# Define flow\n",
        "graph.set_entry_point(\"ClarificationAgent\")\n",
        "graph.add_edge(\"ClarificationAgent\", \"QueryGenerator\")\n",
        "graph.add_edge(\"QueryGenerator\", \"ResearchPipeline\")\n",
        "\n",
        "# Compile the graph into an app object\n",
        "app = graph.compile()\n",
        "print(\"✅ StateGraph compiled successfully.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jf_eFcEq0L3X",
        "outputId": "fc15a043-cafc-432b-da68-df950b335524"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ StateGraph compiled successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cell 9 — Memory initialization and chat() function (single-turn)"
      ],
      "metadata": {
        "id": "cRB3NmoV0ZWh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize memory and provide a chat() function to process one input at a time\n",
        "memory = {\n",
        "    \"facts\": {},    # Persistent knowledge (like user name, facts)\n",
        "    \"history\": []   # Conversation log\n",
        "}\n",
        "\n",
        "def extract_facts_with_gemini(text: str):\n",
        "    \"\"\"\n",
        "    Use Gemini to extract personal facts in JSON-list format: [{\"key\":\"...\", \"value\":\"...\"}]\n",
        "    Fallbacks are safe and non-fatal.\n",
        "    \"\"\"\n",
        "    model = genai.GenerativeModel(\"gemini-1.5-flash\")\n",
        "    prompt = f\"\"\"\n",
        "Extract any personal facts (name, age, location, role, company) from the following user sentence.\n",
        "Return a JSON list of objects with \"key\" and \"value\". If none, return [].\n",
        "\n",
        "Sentence: \"{text}\"\n",
        "\"\"\"\n",
        "    try:\n",
        "        response = model.generate_content(prompt)\n",
        "        text_out = safe_extract_genai_text(response)\n",
        "        # try to parse\n",
        "        facts = []\n",
        "        try:\n",
        "            facts = json.loads(text_out)\n",
        "        except:\n",
        "            # try to find JSON array substring\n",
        "            m = re.search(r\"\\[.*\\]\", text_out, flags=re.DOTALL)\n",
        "            if m:\n",
        "                try:\n",
        "                    facts = json.loads(m.group(0))\n",
        "                except:\n",
        "                    facts = []\n",
        "        if not isinstance(facts, list):\n",
        "            facts = []\n",
        "        return facts\n",
        "    except Exception:\n",
        "        return []\n",
        "\n",
        "def chat(user_input: str, remember_name_rule: bool = True):\n",
        "    \"\"\"\n",
        "    Process a single user_input through the scope pipeline.\n",
        "    Returns the final state dict.\n",
        "    \"\"\"\n",
        "    global memory\n",
        "\n",
        "    # 1) Quick custom name rule\n",
        "    if remember_name_rule and re.search(r\"\\bmy name is\\b\", user_input.lower()):\n",
        "        # extract the phrase after \"my name is\"\n",
        "        try:\n",
        "            name = user_input.lower().split(\"my name is\", 1)[1].strip().split()[0]\n",
        "            memory[\"facts\"][\"name\"] = name.capitalize()\n",
        "            print(f\"✅ Stored name='{memory['facts']['name']}' in memory.\")\n",
        "        except Exception:\n",
        "            pass\n",
        "    else:\n",
        "        # 1b) Use Gemini to detect facts\n",
        "        try:\n",
        "            facts_list = extract_facts_with_gemini(user_input)\n",
        "            for f in facts_list:\n",
        "                key = f.get(\"key\", \"\").lower().strip()\n",
        "                value = f.get(\"value\", \"\").strip()\n",
        "                if key and value:\n",
        "                    memory[\"facts\"][key] = value\n",
        "                    print(f\"✅ I'll remember your {key} = {value}\")\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "    # 2) Handle simple recall commands locally\n",
        "    if user_input.lower().startswith(\"what is my\"):\n",
        "        key = user_input.lower().replace(\"what is my\", \"\").strip()\n",
        "        val = memory[\"facts\"].get(key, \"I don’t know yet.\")\n",
        "        print(f\"Memory: {val}\")\n",
        "        return {\"user_input\": user_input, \"clarification\": \"\", \"query\": \"\", \"pipeline\": \"recall\", \"summary\": val}\n",
        "\n",
        "    # 3) Create state and invoke the pipeline (LangGraph app)\n",
        "    state: ResearchState = {\n",
        "        \"user_input\": user_input,\n",
        "        \"clarification\": \"\",\n",
        "        \"query\": \"\",\n",
        "        \"summary\": \"\",\n",
        "        \"pipeline\": \"\"\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        state = app.invoke(state)\n",
        "    except Exception as e:\n",
        "        traceback.print_exc()\n",
        "        state[\"pipeline\"] = f\"Graph invocation error: {str(e)}\"\n",
        "        state[\"summary\"] = \"\"\n",
        "\n",
        "    # 4) Show outputs\n",
        "    print(\"\\n### 🟢 Clarification Agent\")\n",
        "    print(state.get(\"clarification\", \"\"))\n",
        "    print(\"\\n### 📌 Final Research Query\")\n",
        "    print(state.get(\"query\", \"\"))\n",
        "    print(\"\\n### 🔎 Research Pipeline\")\n",
        "    print(state.get(\"pipeline\", \"\"))\n",
        "    print(\"\\n### ✅ Final Summary\")\n",
        "    print(state.get(\"summary\", \"\"))\n",
        "\n",
        "    # 5) Save to memory/history\n",
        "    memory[\"history\"].append({\n",
        "        \"timestamp\": time.time(),\n",
        "        \"Q\": user_input,\n",
        "        \"clarification\": state.get(\"clarification\", \"\"),\n",
        "        \"query\": state.get(\"query\", \"\"),\n",
        "        \"pipeline\": state.get(\"pipeline\", \"\"),\n",
        "        \"A\": state.get(\"summary\", \"\")\n",
        "    })\n",
        "\n",
        "    return state\n"
      ],
      "metadata": {
        "id": "QRdsfsu90XGu"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cell 10 — continuous chatbot loop"
      ],
      "metadata": {
        "id": "vSZKmyI50xl0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Continuous chatbot loop\n",
        "print(\"🟢 OpenDeepResearcher Chatbot (type 'quit' or 'exit' to stop)\\n\")\n",
        "\n",
        "while True:\n",
        "    try:\n",
        "        user_input = input(\"You: \").strip()\n",
        "        if user_input.lower() in [\"quit\", \"exit\", \"bye\"]:\n",
        "            print(\"👋 Goodbye! Session ended.\")\n",
        "            break\n",
        "\n",
        "        # Process the user input through our chat() pipeline\n",
        "        state = chat(user_input)\n",
        "\n",
        "    except KeyboardInterrupt:\n",
        "        print(\"\\n👋 Interrupted. Goodbye!\")\n",
        "        break\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ Error: {e}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ixntL6zZ01uz",
        "outputId": "628afa0e-e658-459a-ab57-9009f3684bfa"
      },
      "execution_count": 11,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🟢 OpenDeepResearcher Chatbot (type 'quit' or 'exit' to stop)\n",
            "\n",
            "You: what is my name\n",
            "Memory: Priya,\n",
            "You: my name is Nischay\n",
            "✅ Stored name='Nischay' in memory.\n",
            "\n",
            "### 🟢 Clarification Agent\n",
            "This request is clear\n",
            "\n",
            "### 📌 Final Research Query\n",
            "my name is Nischay\n",
            "\n",
            "### 🔎 Research Pipeline\n",
            "Retrieved from memory (facts).\n",
            "\n",
            "### ✅ Final Summary\n",
            "Your name is Nischay.\n",
            "You: where i am from\n",
            "\n",
            "### 🟢 Clarification Agent\n",
            "Could you provide more details about your question?\n",
            "\n",
            "### 📌 Final Research Query\n",
            "where i am from (needs clarification: Could you provide more details about your question?)\n",
            "\n",
            "### 🔎 Research Pipeline\n",
            "Gemini Only\n",
            "\n",
            "### ✅ Final Summary\n",
            "To answer \"Where I am from,\" I need more context.  Are you asking about my origin as a large language model? My location? Or perhaps the location of the person asking the question?  Please specify your query for a relevant response.\n",
            "You: where is my location\n",
            "\n",
            "### 🟢 Clarification Agent\n",
            "Could you provide more details about your question?\n",
            "\n",
            "### 📌 Final Research Query\n",
            "where is my location (needs clarification: Could you provide more details about your question?)\n",
            "\n",
            "### 🔎 Research Pipeline\n",
            "Tavily + Gemini\n",
            "\n",
            "### ✅ Final Summary\n",
            "* The original query \"Where is my location?\" is too vague to answer without further information.  The system needs additional context.\n",
            "* Websites and apps often request location data for personalized services, improved accuracy, and regional information delivery.\n",
            "*  Location services on devices (Android and Apple) utilize GPS, Wi-Fi, cellular data, and sensors for location determination, with varying levels of accuracy depending on the environment and settings.\n",
            "* Some websites may inquire about a user's location for business purposes (e.g., to determine local service availability).\n",
            "*  The provided web results offer information about how location data is used and managed by different services, not a direct answer to the initial location query.\n",
            "You: what is previous que\n",
            "\n",
            "### 🟢 Clarification Agent\n",
            "This request is clear\n",
            "\n",
            "### 📌 Final Research Query\n",
            "what is previous que\n",
            "\n",
            "### 🔎 Research Pipeline\n",
            "Retrieved from memory (history).\n",
            "\n",
            "### ✅ Final Summary\n",
            "Your previous question was: 'where is my location'\n",
            "You: i am from Nainital\n",
            "✅ I'll remember your location = Nainital\n",
            "\n",
            "### 🟢 Clarification Agent\n",
            "Could you provide more details about your question?\n",
            "\n",
            "### 📌 Final Research Query\n",
            "i am from Nainital (needs clarification: Could you provide more details about your question?)\n",
            "\n",
            "### 🔎 Research Pipeline\n",
            "Gemini Only\n",
            "\n",
            "### ✅ Final Summary\n",
            "That's great!  To help me understand your question, please tell me what you'd like to know or discuss about Nainital.  Are you looking for information, sharing an experience, or something else?  More context is needed.\n",
            "You: no\n",
            "\n",
            "### 🟢 Clarification Agent\n",
            "Could you provide more details about your question?\n",
            "\n",
            "### 📌 Final Research Query\n",
            "no (needs clarification: Could you provide more details about your question?)\n",
            "\n",
            "### 🔎 Research Pipeline\n",
            "Gemini Only\n",
            "\n",
            "### ✅ Final Summary\n",
            "The response \"no\" is insufficient.  More information is required to understand the intended question.  The query needs further details or context to provide a meaningful answer.  Please rephrase or elaborate on your request.\n",
            "You: what is ai \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 990.50ms\n",
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 1191.59ms\n",
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-332683201.py\", line 98, in research_pipeline\n",
            "    response = model.generate_content(combined_info)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/google/generativeai/generative_models.py\", line 331, in generate_content\n",
            "    response = self._client.generate_content(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/google/ai/generativelanguage_v1beta/services/generative_service/client.py\", line 835, in generate_content\n",
            "    response = rpc(\n",
            "               ^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/google/api_core/gapic_v1/method.py\", line 131, in __call__\n",
            "    return wrapped_func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/google/api_core/retry/retry_unary.py\", line 294, in retry_wrapped_func\n",
            "    return retry_target(\n",
            "           ^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/google/api_core/retry/retry_unary.py\", line 156, in retry_target\n",
            "    next_sleep = _retry_error_helper(\n",
            "                 ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/google/api_core/retry/retry_base.py\", line 214, in _retry_error_helper\n",
            "    raise final_exc from source_exc\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/google/api_core/retry/retry_unary.py\", line 147, in retry_target\n",
            "    result = target()\n",
            "             ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/google/api_core/timeout.py\", line 130, in func_with_timeout\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/google/api_core/grpc_helpers.py\", line 76, in error_remapped_callable\n",
            "    return callable_(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/google/ai/generativelanguage_v1beta/services/generative_service/transports/rest.py\", line 1161, in __call__\n",
            "    raise core_exceptions.from_http_response(response)\n",
            "google.api_core.exceptions.TooManyRequests: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 50\n",
            "Please retry in 39.222301847s.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "### 🟢 Clarification Agent\n",
            "Could you provide more details about your question?\n",
            "\n",
            "### 📌 Final Research Query\n",
            "what is ai (needs clarification: Could you provide more details about your question?)\n",
            "\n",
            "### 🔎 Research Pipeline\n",
            "Error during research: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 50\n",
            "Please retry in 39.222301847s.\n",
            "\n",
            "### ✅ Final Summary\n",
            "An error occurred while fetching results.\n",
            "You: what is ai\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 1012.53ms\n",
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-218545909.py\", line 36, in clarification_agent\n",
            "    response = model.generate_content(prompt)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/google/generativeai/generative_models.py\", line 331, in generate_content\n",
            "    response = self._client.generate_content(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/google/ai/generativelanguage_v1beta/services/generative_service/client.py\", line 835, in generate_content\n",
            "    response = rpc(\n",
            "               ^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/google/api_core/gapic_v1/method.py\", line 131, in __call__\n",
            "    return wrapped_func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/google/api_core/retry/retry_unary.py\", line 294, in retry_wrapped_func\n",
            "    return retry_target(\n",
            "           ^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/google/api_core/retry/retry_unary.py\", line 156, in retry_target\n",
            "    next_sleep = _retry_error_helper(\n",
            "                 ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/google/api_core/retry/retry_base.py\", line 214, in _retry_error_helper\n",
            "    raise final_exc from source_exc\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/google/api_core/retry/retry_unary.py\", line 147, in retry_target\n",
            "    result = target()\n",
            "             ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/google/api_core/timeout.py\", line 130, in func_with_timeout\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/google/api_core/grpc_helpers.py\", line 76, in error_remapped_callable\n",
            "    return callable_(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/google/ai/generativelanguage_v1beta/services/generative_service/transports/rest.py\", line 1161, in __call__\n",
            "    raise core_exceptions.from_http_response(response)\n",
            "google.api_core.exceptions.TooManyRequests: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 50\n",
            "Please retry in 34.567589593s.\n",
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 1089.91ms\n",
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-332683201.py\", line 105, in research_pipeline\n",
            "    response = model.generate_content(prompt)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/google/generativeai/generative_models.py\", line 331, in generate_content\n",
            "    response = self._client.generate_content(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/google/ai/generativelanguage_v1beta/services/generative_service/client.py\", line 835, in generate_content\n",
            "    response = rpc(\n",
            "               ^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/google/api_core/gapic_v1/method.py\", line 131, in __call__\n",
            "    return wrapped_func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/google/api_core/retry/retry_unary.py\", line 294, in retry_wrapped_func\n",
            "    return retry_target(\n",
            "           ^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/google/api_core/retry/retry_unary.py\", line 156, in retry_target\n",
            "    next_sleep = _retry_error_helper(\n",
            "                 ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/google/api_core/retry/retry_base.py\", line 214, in _retry_error_helper\n",
            "    raise final_exc from source_exc\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/google/api_core/retry/retry_unary.py\", line 147, in retry_target\n",
            "    result = target()\n",
            "             ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/google/api_core/timeout.py\", line 130, in func_with_timeout\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/google/api_core/grpc_helpers.py\", line 76, in error_remapped_callable\n",
            "    return callable_(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/google/ai/generativelanguage_v1beta/services/generative_service/transports/rest.py\", line 1161, in __call__\n",
            "    raise core_exceptions.from_http_response(response)\n",
            "google.api_core.exceptions.TooManyRequests: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 50\n",
            "Please retry in 31.469971691s.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "### 🟢 Clarification Agent\n",
            "This request is clear\n",
            "\n",
            "### 📌 Final Research Query\n",
            "what is ai\n",
            "\n",
            "### 🔎 Research Pipeline\n",
            "Error during research: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 50\n",
            "Please retry in 31.469971691s.\n",
            "\n",
            "### ✅ Final Summary\n",
            "An error occurred while fetching results.\n",
            "You: quit\n",
            "👋 Goodbye! Session ended.\n"
          ]
        }
      ]
    }
  ]
}